# 每日一题

**[daily-question ](https://github.com/amusi/daily-question)**涉及但不局限于机器学习、深度学习和计算机视觉等方向

Warning：众人拾柴火焰高，如果大家有看到很好的题目，可以通过提交issue的方式把题目和答案分享出来，互相学习，一起进步



# 题目

1.【排序题】梯度下降算法的正确步骤是什么？（dcaeb）

# 题目

**1.【排序题】梯度下降算法的正确步骤是什么？（dcaeb）**

a.计算预测值和真实值之间的误差

b.重复迭代，直至得到网络权重的最佳值

c.把输入传入网络，得到输出值

d.用随机值初始化权重和偏差

e.对每一个产生误差的神经元，调整相应的（权重）值以减小误差


**2.【多选题】小明在训练深度学习模型时，发现训练集误差不断减少，测试集误差不断增大，以下解决方法正确的是：（ACD）**

A. 数据增广

B. 增加网络深度

C. 提前停止训练

D. 添加Dropout


**3.【单选题】以下关于鞍点上的Hessian矩阵的描述哪个是正确的？（C）**

A. 正定矩阵

B. 负定矩阵

C. 半正定矩阵

D. 都不对

**4.【单选题】以下几种优化方法中，哪种对超参数最不敏感？（C）**

A. SGD（stochatic gradient descent）

B. BGD（batch gradient descent）

C. Adadetla

D. Momentum

解析：

1）SGD受到学习率α影响

2）BGD受到batch规模m影响

3）Adagrad的一大优势时可以避免手动调节学习率，比如设置初始的缺省学习率为0.01，然后就不管它，另其在学习的过程中自己变化。

为了避免削弱单调猛烈下降的减少学习率，Adadelta产生了1。Adadelta限制把历史梯度累积窗口限制到固定的尺寸w，而不是累加所有的梯度平方和

4）Momentum：也受到学习率α的影响